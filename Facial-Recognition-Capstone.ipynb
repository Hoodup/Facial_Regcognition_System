{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a1540c-644e-4a94-be74-4de467c152ed",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66a54f30-beef-47c2-a50d-6367166f5739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.applications import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d6b3f4-12ea-478c-bd77-7f69a71eaeea",
   "metadata": {},
   "source": [
    "- PLease note that most **print()** statement in my codes were introduced for debugging purpose, i will remove irrelevant ones when i convinced of proper functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338407c8-698f-4ad7-bde1-5fe8283d381c",
   "metadata": {},
   "source": [
    "### Face detection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2dba83f-03c4-4e0b-8fdb-f7a84b779be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Face Detection with HAAR Cascade\n",
    "def detect_faces(image_path):\n",
    "    # Load the HAAR cascade for face detection\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Detect faces\n",
    "    # highest_Neigbor = 0\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    # Create a directory for the new image\n",
    "    working_dir = os.getcwd()\n",
    "    folder_path =os.path.join(working_dir, 'cropped')\n",
    "    \n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    # If faces are detected, crop and save them\n",
    "    cropped_faces = []\n",
    "    for (x, y, w, h) in faces:\n",
    "        cropped_face = image[y:y+h, x:x+w]\n",
    "        cropped_faces.append(cropped_face)\n",
    "        # Save the cropped face\n",
    "        # cv2.imwrite(f\"{os.path.basename(image_path)}\", cropped_face)\n",
    "        cv2.imwrite(os.path.join(folder_path, os.path.basename(image_path)), cropped_face)\n",
    "\n",
    "    \n",
    "    return cropped_faces\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd5f40-7fd0-429c-a1de-aaee7578905c",
   "metadata": {},
   "source": [
    "# VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "921642df-4b40-479b-bb16-f22fdc7d0898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initialize VGG16\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "vgg16_model = VGG16(weights='imagenet', include_top=False, pooling='avg')  # Use average pooling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7742a975-d773-470a-828d-d94f46e9a22f",
   "metadata": {},
   "source": [
    "### Get Embeddings Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecc0bfe8-65dd-40b2-a89e-da8ce7772520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate Embeddings using Pre-trained VGG16 Model\n",
    "def get_embeddings(face_images, model):\n",
    "    embeddings = []\n",
    "    \n",
    "    for face in face_images:\n",
    "        # Resize and preprocess the face image\n",
    "        face_resized = cv2.resize(face, (160, 160))  # Resize for VGG16 input\n",
    "        face_array = image.img_to_array(face_resized)\n",
    "        face_array = np.expand_dims(face_array, axis=0)\n",
    "        face_array = preprocess_input(face_array)\n",
    "        \n",
    "        # Generate embedding\n",
    "        embedding = model.predict(face_array)\n",
    "        embeddings.append(embedding.flatten())  # Flatten the output to 1D\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dea420-22a1-4468-97b2-55bed28fba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_npz_file():\n",
    "    for path in the_cropped_paths:\n",
    "        img = cv2.imread(path)\n",
    "        train_images.append(img)\n",
    "    train_images = np.array(train_images)\n",
    "    np.savez('cropped_faces', train = train_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b07defd-84ae-4b50-ac1d-19ea3ec32283",
   "metadata": {},
   "source": [
    "### SVM training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55c06b22-82f8-4fb9-8032-28e6538fda0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train SVM Classifier\n",
    "def train_svm(embeddings, labels):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize and train the SVM classifier\n",
    "    clf = svm.SVC(kernel='linear', probability=True)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the classifier\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    # print(r2score(y_test, y_pred))\n",
    "\n",
    "    print(\"Train Score: \",clf.score(X_train, y_train))\n",
    "    \n",
    "    \n",
    "    return clf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a01d3e9-4346-4b08-b263-e2429b709bce",
   "metadata": {},
   "source": [
    "### Model saving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40837d2b-f22f-44c0-9106-7609f08b8c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Save the Model \n",
    "def save_model(clf, filename='svm_model.pkl'):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(clf, f)\n",
    "    print(f\"Model saved as {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3a79466-f019-4be4-8f79-b38888714ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Classify New Faces with the Trained Model\n",
    "def classify_face(image_path, clf):\n",
    "    faces = detect_faces(image_path)  # Detect faces in the image\n",
    "    embeddings = get_embeddings(face_images=faces, model=vgg16_model)  # Generate embeddings for each face\n",
    "    \n",
    "    # Classify each face\n",
    "    for embedding in embeddings:\n",
    "        prediction = clf.predict([embedding])  # Predict using the trained model\n",
    "        print(f\"Predicted Label: {prediction[0]}\")\n",
    "        print(f\"Predicted Label: {type(prediction[0])}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fd2a1b-b45a-45bc-92e9-24bdbb2f86ff",
   "metadata": {},
   "source": [
    "### Load Your Dataset\n",
    "Loading my dataset from my_faces/ directory, asking glob to request for every image with the extension .jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efd6af9-d1da-462b-837c-a3990e0a8ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my_faces\\\\person_1_1.jpg', 'my_faces\\\\person_1_2.jpg', 'my_faces\\\\person_1_3.jpg', 'my_faces\\\\person_1_4.jpg', 'my_faces\\\\person_1_5.jpg', 'my_faces\\\\person_1_6.jpg', 'my_faces\\\\person_2_1.jpg', 'my_faces\\\\person_2_10.jpg', 'my_faces\\\\person_2_11.jpg', 'my_faces\\\\person_2_12 (2).jpg', 'my_faces\\\\person_2_12 (3).jpg', 'my_faces\\\\person_2_12 (4).jpg', 'my_faces\\\\person_2_12 (5).jpg', 'my_faces\\\\person_2_12 (6).jpg', 'my_faces\\\\person_2_12 (7).jpg', 'my_faces\\\\person_2_12 (8).jpg', 'my_faces\\\\person_2_12 (9).jpg', 'my_faces\\\\person_2_12.jpg', 'my_faces\\\\person_2_13.jpg', 'my_faces\\\\person_2_14.jpg', 'my_faces\\\\person_2_15.jpg', 'my_faces\\\\person_2_16.jpg', 'my_faces\\\\person_2_17.jpg', 'my_faces\\\\person_2_19.jpg', 'my_faces\\\\person_2_2.jpg', 'my_faces\\\\person_2_20.jpg', 'my_faces\\\\person_2_3.jpg', 'my_faces\\\\person_2_4.jpg', 'my_faces\\\\person_2_5.jpg', 'my_faces\\\\person_2_6.jpg', 'my_faces\\\\person_2_7.jpg', 'my_faces\\\\person_2_8.jpg', 'my_faces\\\\person_2_9.jpg', 'my_faces\\\\person_3_1.jpg', 'my_faces\\\\person_3_2.jpg', 'my_faces\\\\person_3_3.jpg', 'my_faces\\\\person_3_4.jpg', 'my_faces\\\\person_3_5.jpg', 'my_faces\\\\person_3_6.jpg', 'my_faces\\\\person_4_1.jpg', 'my_faces\\\\person_4_2.jpg', 'my_faces\\\\person_4_3.jpg', 'my_faces\\\\person_4_4.jpg', 'my_faces\\\\person_4_5.jpg', 'my_faces\\\\person_5_1.jpg', 'my_faces\\\\person_5_2.jpg', 'my_faces\\\\person_6_1.jpg', 'my_faces\\\\person_6_10.jpg', 'my_faces\\\\person_6_11.jpg', 'my_faces\\\\person_6_12.jpg', 'my_faces\\\\person_6_13.jpg', 'my_faces\\\\person_6_14.jpg', 'my_faces\\\\person_6_16.jpg', 'my_faces\\\\person_6_2.jpg', 'my_faces\\\\person_6_3.jpg', 'my_faces\\\\person_6_4.jpg', 'my_faces\\\\person_6_5.jpg', 'my_faces\\\\person_6_6.jpg', 'my_faces\\\\person_6_7.jpg', 'my_faces\\\\person_6_8.jpg', 'my_faces\\\\person_6_9.jpg']\n",
      "my_faces\\person_1_1.jpg\n",
      "---------------------------- ['person', '1', '1.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Example: Define paths to your images (small dataset)\n",
    "image_paths = glob.glob(\"my_faces/*.jpg\")  # Tried using glob to load image from a folder\n",
    "print(image_paths)\n",
    "\n",
    "# Collect faces and generate embeddings\n",
    "\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "actual_names = ['Tomiwa', 'Samuel', 'Ransome', 'Ifechukwu', 'Seun', 'Christabel' ]\n",
    "current_label = 0\n",
    "\n",
    "label_map = {}  # Example label mapping\n",
    "the_cropped_paths = []\n",
    "train_images =[]\n",
    "\n",
    "for image_path in image_paths:\n",
    "    print(image_path)\n",
    "    label_name = os.path.basename(image_path).split('_')  # Assuming image names follow format 'person_1_x.jpg'\n",
    "    print(\"----------------------------\",label_name)\n",
    "    label_name=label_name[0]+\"_\"+label_name[1]\n",
    "    the_cropped_paths.append('cropped/'+label_name)\n",
    "    # print(label_name.split('.')[0])\n",
    "\n",
    "    if label_name not in label_map:\n",
    "        label_map[f'{label_name}'] = current_label\n",
    "        current_label += 1  # Increment label number\n",
    "    faces = detect_faces(image_path)\n",
    "    embeddings = get_embeddings(face_images=faces, model=vgg16_model)\n",
    "    all_embeddings.extend(embeddings)\n",
    "    all_labels.extend([label_map[label_name]] * len(embeddings))  # Assign the label for each face\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb76f6-3e1a-4b72-a591-a1f9a5859211",
   "metadata": {},
   "source": [
    "5. Train the SVM Classifier\n",
    "In the next cell, train the SVM classifier using the generated embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46c20a61-6497-43e7-bcf9-57b955d16d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'person_1': 0,\n",
       " 'person_2': 1,\n",
       " 'person_3': 2,\n",
       " 'person_4': 3,\n",
       " 'person_5': 4,\n",
       " 'person_6': 5}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4595344-9ac7-4b78-b608-54c62770ce95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         2\n",
      "           4       1.00      1.00      1.00         1\n",
      "           5       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00        10\n",
      "   macro avg       1.00      1.00      1.00        10\n",
      "weighted avg       1.00      1.00      1.00        10\n",
      "\n",
      "Train Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Train the SVM Classifier\n",
    "clf = train_svm(all_embeddings, all_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dfbc0c-1bda-4f01-9d9b-0e34d913496c",
   "metadata": {},
   "source": [
    "6. Save the Model (Optional)\n",
    "If you want to save the trained model for later use, execute this in a new cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ad2a342-046a-42cd-9ef9-a037daac376b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as svm_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Optional: Save the trained model\n",
    "save_model(clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7465afbb-4635-43a5-8108-196af107c25f",
   "metadata": {},
   "source": [
    "7. Test the Classifier on New Images\n",
    "Finally, use the classifier to classify a new image. In a new cell, call the classify_face function with a test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc61e470-cfba-4a13-8481-8a30f7991f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted Label: 1\n",
      "Predicted Label: <class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Classify a new image (example)\n",
    "test_image = 'my_faces/WIN_20250331_10_02_42_Pro.jpg'  # Replace with your test image path\n",
    "classify_face(test_image, clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e72871c4-fdc4-40f2-9b0a-ffc1865a68c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6151b222-b22c-48f5-bae8-991a093a1ad4",
   "metadata": {},
   "source": [
    "Summary of Notebook Cells\n",
    "Cell 1: Install dependencies (pip install ...).\n",
    "\n",
    "Cell 2: Import necessary libraries.\n",
    "\n",
    "Cell 3: Define helper functions for face detection, embedding generation, SVM training, and saving the model.\n",
    "\n",
    "Cell 4: Load images and preprocess them (detect faces, generate embeddings).\n",
    "\n",
    "Cell 5: Train the SVM classifier.\n",
    "\n",
    "Cell 6: Save the trained model (optional).\n",
    "\n",
    "Cell 7: Test the classifier on a new image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb14f32-b6ef-4a2c-bc1b-934811971a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5775c0a-5259-45fb-93ed-9bd7a1268426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbbc7e6-2935-429f-a0e1-7d576ec4653e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f61e2ee-60c5-413c-9750-c205ccbc820c",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (facerec)",
   "language": "python",
   "name": "facerec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
